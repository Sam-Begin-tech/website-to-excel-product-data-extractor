# ğŸ›’ Product Data Extractor from Website to Excel (LLM-Powered)

This is a Python-based prototype that automatically crawls a website, extracts product-related information using an AI language model, and saves the results into an Excel spreadsheet.

Itâ€™s designed to be simple and beginner-friendly, using modular Python code and LLM (Large Language Model) via LangChain + Ollama for intelligent text understanding.

---

## âœ¨ What It Does

âœ… Crawls a website and visits all internal links  
âœ… Extracts all visible text from each page  
âœ… Cleans the content by removing unnecessary text like â€œLoginâ€, â€œAdd to Cartâ€, etc.  
âœ… Breaks large content into chunks  
âœ… Uses a Language Model (like Gemma via Ollama) to extract:
- Product Name
- MRP (Maximum Retail Price)
- Selling Price  
âœ… Parses LLM output into clean structured JSON  
âœ… Removes duplicate products  
âœ… Saves the final list into an Excel file

---

## ğŸ–¼ï¸ Example Output (Excel Columns)

| ProductName         | MRP    | sellingPrice |
|---------------------|--------|---------------|
| Sunflower Oil 1L    | â‚¹200   | â‚¹180          |
| Aashirvaad Atta 5kg | â‚¹300   | â‚¹270          |

---

## ğŸ§  Tech Stack

- Python 3
- requests + BeautifulSoup â€“ for web crawling
- re + json â€“ for cleaning and formatting
- LangChain + Ollama â€“ for LLM-driven extraction
- pandas + numpy â€“ for data processing
- Excel Export â€“ via pandas `.to_excel()`

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Ollama installed locally with model like `gemma2:2b`
- LangChain library
- pandas, numpy, beautifulsoup4


| File              | Purpose                              |
| ----------------- | ------------------------------------ |
| product_data_extraction.ipynb           | Main logic for crawling & extraction |
|data_extraction.ipynb |  Notebook showing your initial prototyping|
| README.md         | This documentation                   |
| products.xlsx | Output file (auto-generated)         |


---

## ğŸ§  Ollama Installation & Model Setup

This project uses Ollama to run a local LLM (Large Language Model) that extracts product details from raw webpage text.

### ğŸ“¥ Step 1: Install Ollama

Ollama allows you to run open-source LLMs locally on your machine.

Install it from the official site:

ğŸ”— [https://ollama.com/download](https://ollama.com/download)

Choose the version for:

* Windows
* macOS
* Linux (Ubuntu/Debian via terminal)

After installation, make sure the ollama command is available in your terminal:

```bash
ollama --version
---

## ğŸ§  Ollama Installation & Model Setup

This project uses Ollama to run a local LLM (Large Language Model) that extracts product details from raw webpage text.

### ğŸ“¥ Step 1: Install Ollama

Ollama allows you to run open-source LLMs locally on your machine.

Install it from the official site:

ğŸ”— [https://ollama.com/download](https://ollama.com/download)

Choose the version for:

* Windows
* macOS
* Linux (Ubuntu/Debian via terminal)

After installation, make sure the ollama command is available in your terminal:

```bash
ollama --version
```

If this works, you're ready to run local models!

---

### ğŸ“¦ Step 2: Pull the Required Model

This project uses the Gemma 2B model. Pull it using:

```bash
ollama pull gemma2:2b
```

Wait for the model to download (this may take a few minutes). Once completed, Ollama can run it locally in the background.

You can test it with:

```bash
ollama run gemma2:2b
```

---

ğŸ“ Note:
You can replace gemma2:2b with any other LLM you prefer (e.g., phi3, llama3, etc.), as long as it's compatible with your LangChain or inference code.

---
```

If this works, you're ready to run local models!

---

### ğŸ“¦ Step 2: Pull the Required Model

This project uses the Gemma 2B model. Pull it using:

```bash
ollama pull gemma2:2b
```

Wait for the model to download (this may take a few minutes). Once completed, Ollama can run it locally in the background.

You can test it with:

```bash
ollama run gemma2:2b
```

---

ğŸ“ Note:
You can replace gemma2:2b with any other LLM you prefer (e.g., phi3, llama3, etc.), as long as it's compatible with your LangChain or inference code.

---

---

## ğŸŒ Demo Website Used

This prototype was tested using the following publicly accessible e-commerce site:

ğŸ”—â€‚[https://edenmart.in/](https://edenmart.in/)

It was used as an example to:

* Demonstrate product data crawling and extraction
* Validate LLM-based parsing of pricing details
* Export a real-world dataset into Excel

âš ï¸ Note:
This tool is intended solely for educational and demonstration purposes. It does not store, misuse, or distribute any data obtained from the target website. Users are strongly advised to review and comply with the websiteâ€™s robots.txt file and Terms of Service before applying this tool to any external domain.

---

Install dependencies:

```bash
pip install requests beautifulsoup4 langchain langchain-community pandas numpy openpyxl
